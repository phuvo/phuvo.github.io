# Misc terms

## Normalization

- [Why do transformers use layer norm instead of batch norm?](https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm)

## Zero-shot, one-shot, few-shot

See [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
